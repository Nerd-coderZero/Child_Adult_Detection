import streamlit as st
import cv2
import torch
import torch.nn as nn
from torchvision import models
import numpy as np
from pathlib import Path
from deep_sort_realtime.deepsort_tracker import DeepSort
import logging
from typing import Tuple, List, Dict, Optional
import tempfile
from PIL import Image
import av
from streamlit_webrtc import webrtc_streamer, WebRtcMode, RTCConfiguration
import os
import sys

# Initialize session state
if 'tracker' not in st.session_state:
    st.session_state.tracker = None
if 'processed_video' not in st.session_state:
    st.session_state.processed_video = None
if 'frame_buffer' not in st.session_state:
    st.session_state.frame_buffer = []

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class EnhancedPersonTracker:
    def __init__(
            self,
            model_path: str = 'best_model.pth',
            confidence_threshold: float = 0.75,  # Increased confidence threshold
            input_size: tuple = (416, 416),
            close_up_min_pixels: int = 60,  # Increased minimum size
            close_up_max_pixels: int = 250,  # Reduced maximum size
            close_up_aspect_ratio_bounds: Tuple[float, float] = (0.4, 1.8),  # Tightened bounds
            boundary_margin_ratio: float = 0.15,  # Increased margin
            buffer_size: int = 5  # Added frame buffer size
        ):
            # Setup logging and device
            logging.basicConfig(level=logging.INFO)
            self.logger = logging.getLogger(__name__)
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
            self.logger.info(f"Using device: {self.device}")

            self.input_size = input_size
            self.buffer_size = buffer_size
            self.frame_buffer = []

            # Optimized parameters for false positive reduction
            self.detection_params = {
                'max_size_ratio': 0.80,  # Reduced to avoid large false detections
                'min_pixels': 80,  # Increased minimum size
                'aspect_ratio_bounds': (0.3, 2.0),  # Tightened aspect ratio bounds
                'padding_ratio': 0.05,  # Reduced padding
                'temporal_smoothing': 7,  # Increased temporal smoothing
                'min_detection_confidence': 0.6,  # Minimum detection confidence
                'track_stability_threshold': 5  # Minimum frames for stable track
            }

            # Enhanced tracking parameters
            self.tracking_params = {
                'velocity_persistence': 0.7,  # How much to consider previous velocity
                'position_smoothing': 0.8,  # Position smoothing factor
                'min_hits': 3,  # Minimum hits before establishing track
                'max_age': 20,  # Maximum frames to keep lost tracks
                'min_iou': 0.3  # Minimum IOU for track association
            }

            self.close_up_min_pixels = close_up_min_pixels
            self.close_up_max_pixels = close_up_max_pixels
            self.close_up_aspect_ratio_bounds = close_up_aspect_ratio_bounds
            self.confidence_threshold = confidence_threshold
            self.boundary_margin_ratio = boundary_margin_ratio

            # Enhanced classification history with confidence weighting
            self.classification_history = {}
            self.track_stability = {}

            try:
                self.classification_model = self._load_model(model_path)
                self.detector = self._load_detector()
                self._initialize_tracker()
            except Exception as e:
                self.logger.error(f"Error initializing models: {e}")
                raise

    def _initialize_tracker(self):
        """Initialize DeepSort tracker with optimized parameters"""
        self.tracker = DeepSort(
            max_age=self.tracking_params['max_age'],
            n_init=self.tracking_params['min_hits'],
            max_iou_distance=1.0 - self.tracking_params['min_iou'],
            max_cosine_distance=0.25,  # Stricter appearance matching
            nn_budget=100,
            override_track_class=None,
            embedder=None,
            half=True,  # Use half precision for faster processing
            bgr=True,
            embedder_gpu=True
        )

    def _update_track_stability(self, track_id: int, detection_confidence: float):
        """Update track stability metrics"""
        if track_id not in self.track_stability:
            self.track_stability[track_id] = {
                'consecutive_detections': 0,
                'total_detections': 0,
                'average_confidence': 0
            }

        stability = self.track_stability[track_id]
        stability['consecutive_detections'] += 1
        stability['total_detections'] += 1
        stability['average_confidence'] = (
            stability['average_confidence'] * (stability['total_detections'] - 1) +
            detection_confidence
        ) / stability['total_detections']

    def _is_track_stable(self, track_id: int) -> bool:
        """Check if a track is stable enough for classification"""
        if track_id not in self.track_stability:
            return False

        stability = self.track_stability[track_id]
        return (
            stability['consecutive_detections'] >= self.detection_params['track_stability_threshold'] and
            stability['average_confidence'] >= self.detection_params['min_detection_confidence']
        )

    def process_video(self, video_path: str, output_path: Optional[str] = None,
                     show_display: bool = True):
        """Process video with enhanced tracking and stability checks"""
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            self.logger.error(f"Could not open video file {video_path}")
            return

        frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        fps = int(cap.get(cv2.CAP_PROP_FPS))

        if output_path:
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))

        try:
            while cap.isOpened():
                ret, frame = cap.read()
                if not ret:
                    break

                # Update frame buffer for temporal consistency
                self.frame_buffer.append(frame.copy())
                if len(self.frame_buffer) > self.buffer_size:
                    self.frame_buffer.pop(0)

                # Get detections with temporal consistency check
                detections = self._get_consistent_detections(frame)
                
                if len(detections) > 0:
                    # Convert detections to DeepSort format
                    detection_list = []
                    for det in detections:
                        detection_list.append(([det[0], det[1], det[2] - det[0], det[3] - det[1]], det[4], 'person'))

                    # Update tracks with enhanced stability checking
                    tracks = self.tracker.update_tracks(detection_list, frame=frame)

                    for track in tracks:
                        if not track.is_confirmed():
                            continue

                        track_id = track.track_id
                        ltwh = track.to_ltwh()
                        bbox = np.array([
                            ltwh[0], ltwh[1],
                            ltwh[0] + ltwh[2], ltwh[1] + ltwh[3]
                        ])

                        # Update track stability
                        self._update_track_stability(track_id, track.get_det_conf())

                        # Only process stable tracks
                        if self._is_track_stable(track_id):
                            # Check for significant overlap with other tracks
                            if not self._has_significant_overlap(bbox, tracks, track_id):
                                label, confidence = self.predict_person(frame, bbox, track_id)
                                if label is not None:
                                    self.draw_detection(frame, bbox, track_id, label, confidence)

                if show_display:
                    cv2.imshow('Tracking', frame)
                    if cv2.waitKey(1) & 0xFF == ord('q'):
                        break

                if output_path:
                    out.write(frame)

        except Exception as e:
            self.logger.error(f"Error processing video: {e}")
            raise
        finally:
            cap.release()
            if output_path:
                out.release()
            if show_display:
                cv2.destroyAllWindows()

    def _get_consistent_detections(self, frame: np.ndarray) -> np.ndarray:
        """Get detections with temporal consistency check"""
        current_detections = self.detect_persons(frame)
        
        if len(self.frame_buffer) < 2:
            return current_detections

        # Compare with previous frame detections
        prev_detections = self.detect_persons(self.frame_buffer[-2])
        
        consistent_detections = []
        for det in current_detections:
            # Check if detection has corresponding detection in previous frame
            if self._has_matching_detection(det, prev_detections):
                consistent_detections.append(det)

        return np.array(consistent_detections)

    def _has_matching_detection(self, detection: np.ndarray, prev_detections: np.ndarray,
                              iou_threshold: float = 0.3) -> bool:
        """Check if detection has a match in previous frame"""
        if len(prev_detections) == 0:
            return False

        # Calculate IoU with all previous detections
        ious = np.array([self._calculate_iou(detection[:4], prev_det[:4]) 
                        for prev_det in prev_detections])
        
        return np.any(ious > iou_threshold)

    def _has_significant_overlap(self, bbox: np.ndarray, tracks: List, track_id: int,
                               overlap_threshold: float = 0.3) -> bool:
        """Check for significant overlap with other tracks"""
        for other_track in tracks:
            if other_track.track_id != track_id and other_track.is_confirmed():
                other_ltwh = other_track.to_ltwh()
                other_bbox = np.array([
                    other_ltwh[0], other_ltwh[1],
                    other_ltwh[0] + other_ltwh[2], other_ltwh[1] + other_ltwh[3]
                ])
                
                if self._calculate_iou(bbox, other_bbox) > overlap_threshold:
                    return True
        return False

# Rest of the code remains the same...
